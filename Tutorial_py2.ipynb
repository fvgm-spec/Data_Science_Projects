{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Acquistion\n",
    "--------------------------\n",
    "BigGorilla recommends a list of tools for different data acquisition tasks (See [here]()). Among these tools, **urllib** is a popular python package for fetching data across the web. In this part, we use **urllib** to download the datasets that we need for this tutorial.\n",
    "\n",
    "### Step 1: downloading the \"Kaggle 5000 Movie Dataset\"\n",
    "The desired dataset is a _.csv_ file with a url that is specified in the code snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing urlib (BigGorilla's recommendation for data acquisition from the web)\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "# Creating the data folder\n",
    "if not os.path.exists('./data'):\n",
    "    os.makedirs('./data')\n",
    "\n",
    "# Obtaining the dataset using the url that hosts it\n",
    "kaggle_url = 'https://github.com/sundeepblue/movie_rating_prediction/raw/master/movie_metadata.csv'\n",
    "if not os.path.exists('./data/kaggle_dataset.csv'):     # avoid downloading if the file exists\n",
    "    response = urllib.urlretrieve(kaggle_url, './data/kaggle_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: downloading the \"IMDB Plain Text Data\"\n",
    "The IMDB Plain Text Data (see [here](ftp://ftp.funet.fi/pub/mirrors/ftp.imdb.com/pub/)) is a collection of files where each files describe one or a few attributes of a movie. We are going to focus on a subset of movie attribues which subsequently means that we are only interested in a few of these files which are listed below:\n",
    "\n",
    "* genres.list.gz\n",
    "* ratings.list.gz\n",
    "\n",
    "_** Note: The total size of files mentioned above is roughly 30M. Running the following code may take a few minutes._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "# Obtaining IMDB's text files\n",
    "imdb_url_prefix = 'ftp://ftp.funet.fi/pub/mirrors/ftp.imdb.com/pub/'\n",
    "imdb_files_list = ['genres.list.gz', 'ratings.list.gz']\n",
    "for name in imdb_files_list:\n",
    "    if not os.path.exists('./data/' + name):\n",
    "        response = urllib.urlretrieve(imdb_url_prefix + name, './data/' + name)\n",
    "        urllib.urlcleanup()   # urllib fails to download two files from a ftp source. This fixes the bug!\n",
    "        with gzip.open('./data/' + name) as comp_file, open('./data/' + name[:-3], 'w') as reg_file:\n",
    "            file_content = comp_file.read()\n",
    "            reg_file.write(file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: downloading the \"IMDB Prepared Data\"\n",
    "During this tutorial, we discuss how the contents of _genres.list.gz_ and _ratings.list.gz_ files can be integrated. However, to make the tutorial more concise, we avoid including the same process for all the files in the \"IMDB Plain Text Data\". The \"IMDB Prepared Data\" is the dataset that we obtained by integrating a number of files from the \"IMDB Plain Text Data\" which we will use during later stages of this tutorial. The following code snippet downloads this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb_url = 'https://anaconda.org/BigGorilla/datasets/1/download/imdb_dataset.csv'\n",
    "if not os.path.exists('./data/imdb_dataset.csv'):     # avoid downloading if the file exists\n",
    "    response = urllib.urlretrieve(kaggle_url, './data/imdb_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Data Extraction\n",
    "-----------------\n",
    "The \"Kaggle 5000 Movie Dataset\" is stored in a _.csv_ file which is alreday structured and ready to use. On the other hand, the \"IMDB Plain Text Data\" is a collection of semi-structured text files that need to be processed to extract the data. A quick look at the first few lines of each files shows that each file has a different format and has to be handled separately.\n",
    "\n",
    "##### Content of \"ratings.list\" data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0000000125  1728818   9.2  The Shawshank Redemption (1994)\n",
      "      0000000125  1181412   9.2  The Godfather (1972)\n",
      "      0000000124  810055   9.0  The Godfather: Part II (1974)\n",
      "      0000000124  1714042   8.9  The Dark Knight (2008)\n",
      "      0000000133  461310   8.9  12 Angry Men (1957)\n",
      "      0000000133  885509   8.9  Schindler's List (1993)\n",
      "      0000000123  1354135   8.9  Pulp Fiction (1994)\n",
      "      0000000124  1241908   8.9  The Lord of the Rings: The Return of the King (2003)\n",
      "      0000000123  514540   8.9  Il buono, il brutto, il cattivo (1966)\n",
      "      0000000133  1380148   8.8  Fight Club (1999)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/ratings.list\") as myfile:\n",
    "    head = [next(myfile) for x in range(38)]\n",
    "print (''.join(head[28:38]))   # skipping the first 28 lines as they are descriptive headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Content of the \"genres.list\" data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"!Next?\" (1994)\t\t\t\t\t\tDocumentary\n",
      "\"#1 Single\" (2006)\t\t\t\t\tReality-TV\n",
      "\"#15SecondScare\" (2015)\t\t\t\t\tHorror\n",
      "\"#15SecondScare\" (2015)\t\t\t\t\tShort\n",
      "\"#15SecondScare\" (2015)\t\t\t\t\tThriller\n",
      "\"#15SecondScare\" (2015) {Who Wants to Play with the Rabbit? (#1.2)}\tDrama\n",
      "\"#15SecondScare\" (2015) {Who Wants to Play with the Rabbit? (#1.2)}\tHorror\n",
      "\"#15SecondScare\" (2015) {Who Wants to Play with the Rabbit? (#1.2)}\tShort\n",
      "\"#15SecondScare\" (2015) {Who Wants to Play with the Rabbit? (#1.2)}\tThriller\n",
      "\"#1MinuteNightmare\" (2014)\t\t\t\tHorror\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/genres.list\") as myfile:\n",
    "    head = [next(myfile) for x in range(392)]\n",
    "print (''.join(head[382:392]))   # skipping the first 382 lines as they are descriptive header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Extracting the information from \"genres.list\"\n",
    "The goal of this step is to extract the movie titles and their production year from \"movies.list\", and store the extracted data into a dataframe. Dataframe (from the python package **pandas**) is one of the key BigGorilla's recommendation for data profiling and cleaning. To extract the desired information from the text, we rely on **regular expressions** which are implemented in the python package \"**re**\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"./data/genres.list\") as genres_file:\n",
    "    raw_content = genres_file.readlines()\n",
    "    genres_list = []\n",
    "    content = raw_content[382:]\n",
    "    for line in content:\n",
    "        m = re.match(r'\"?(.*[^\"])\"? \\(((?:\\d|\\?){4})(?:/\\w*)?\\).*\\s((?:\\w|-)+)', line.strip())\n",
    "        genres_list.append([m.group(1), m.group(2), m.group(3)])\n",
    "    genres_data = pd.DataFrame(genres_list, columns=['movie', 'year', 'genre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Extracting the information from \"ratings.list\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"./data/ratings.list\") as ratings_file:\n",
    "    raw_content = ratings_file.readlines()\n",
    "    ratings_list = []\n",
    "    content = raw_content[28:]\n",
    "    for line in content:\n",
    "        m = re.match(r'(?:\\d|\\.|\\*){10}\\s+\\d+\\s+(1?\\d\\.\\d)\\s\"?(.*[^\"])\"? \\(((?:\\d|\\?){4})(?:/\\w*)?\\)', line.strip())\n",
    "        if m is None: continue\n",
    "        ratings_list.append([m.group(2), m.group(3), m.group(1)])\n",
    "    ratings_data = pd.DataFrame(ratings_list, columns=['movie', 'year', 'rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that one has to repeat the information extraction procedure for other data files as well if he is interested in their content. For now (and to keep the tutorial simple), we assume that we are only interested in genres and ratings of movies. The above code snippets store the extracted data on these two attributes into two dataframes (namely, **genres_list** and **ratings_list**).\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Data Profiling & Cleaning\n",
    "---------------------------\n",
    "\n",
    "The high-level goal in this stage of data prepration is to look into the data that we have acquired and extracted so far. This helps us to get familiar with data, understand in what ways the data needs cleaning or transformation, and finally enables us to prepare the data for the following steps of the data integration task.\n",
    "\n",
    "### Step 1: Loading the \"Kaggle 5000 Movies Dataset\"\n",
    "\n",
    "According to BigGorilla, dataframes (from the python package **pandas**) are suitable for data exploration and data profiling. In [Part 2](https://github.com/rit-git/BigGorilla/blob/tutorial/Tutorial/Part%202%20--%20Data%20Extraction.ipynb) of the tutorial, we stored the extracted data from \"IMDB Plain Text Data\" into dataframes. It would be appropriate to load the \"Kaggle 5000 Movies Dataset\" into a dataframe as well and follow the same data profiling procedure for all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the Kaggle dataset from the .csv file (kaggle_dataset.csv)\n",
    "kaggle_data = pd.read_csv('./data/kaggle_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Calculating some basic statistics (profiling)\n",
    "\n",
    "Let's start by finding out how many movies are listed in each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies in kaggle_data: 5043\n",
      "Number of movies in genres_data: 2384400\n",
      "Number of movies in ratings_data: 691621\n"
     ]
    }
   ],
   "source": [
    "print ('Number of movies in kaggle_data: {}'.format(kaggle_data.shape[0]))\n",
    "print ('Number of movies in genres_data: {}'.format(genres_data.shape[0]))\n",
    "print ('Number of movies in ratings_data: {}'.format(ratings_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check to see if we have duplicates (i.e., a movie appearing more than once) in the data. We consider an entry duplicate if we can find another entry with the same movie title and production year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in kaggle_data: 241\n",
      "Number of duplicates in genres_data: 1807712\n",
      "Number of duplicates in ratings_data: 286515\n"
     ]
    }
   ],
   "source": [
    "print ('Number of duplicates in kaggle_data: {}'.format(\n",
    "    sum(kaggle_data.duplicated(subset=['movie_title', 'title_year'], keep=False))))\n",
    "print ('Number of duplicates in genres_data: {}'.format(\n",
    "    sum(genres_data.duplicated(subset=['movie', 'year'], keep=False))))\n",
    "print ('Number of duplicates in ratings_data: {}'.format(\n",
    "    sum(ratings_data.duplicated(subset=['movie', 'year'], keep=False))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Dealing with duplicates (cleaning)\n",
    "\n",
    "There are many strategies to deal with duplicates. Here, we are going to use a simple method for dealing with duplicates and that is to only keep the first occurrence of a duplicated entry and remove the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kaggle_data = kaggle_data.drop_duplicates(subset=['movie_title', 'title_year'], keep='first').copy()\n",
    "genres_data = genres_data.drop_duplicates(subset=['movie', 'year'], keep='first').copy()\n",
    "ratings_data = ratings_data.drop_duplicates(subset=['movie', 'year'], keep='first').copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Normalizing the text (cleaning)\n",
    "\n",
    "The key attribute that we will use to integrate our movie datasets is the movie titles. So it is important to normalize these titles. The following code snippet makes all movie titles lower case, and then removes certain characters such as \"'\" and \"?\", and replaces some other special characters (e.g., \"&\" is replaced with \"and\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_title(title):\n",
    "    title = title.lower()\n",
    "    title = title.replace(',', ' ')\n",
    "    title = title.replace(\"'\", '')    \n",
    "    title = title.replace('&', 'and')\n",
    "    title = title.replace('?', '')\n",
    "    title = title.decode('utf-8', 'ignore')\n",
    "    return title.strip()\n",
    "\n",
    "kaggle_data['norm_movie_title'] = kaggle_data['movie_title'].map(preprocess_title)\n",
    "genres_data['norm_movie'] = genres_data['movie'].map(preprocess_title)\n",
    "ratings_data['norm_movie'] = ratings_data['movie'].map(preprocess_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Looking at a few samples\n",
    "\n",
    "The goal here is to a look at a few sample entries from each dataset for a quick sanity check. To keep the tutorial consice, we just present this step for the \"Kaggle 5000 Movies Dataset\" which is stored in the **kaggle_data** dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>director_name</th>\n",
       "      <th>num_critic_for_reviews</th>\n",
       "      <th>duration</th>\n",
       "      <th>director_facebook_likes</th>\n",
       "      <th>actor_3_facebook_likes</th>\n",
       "      <th>actor_2_name</th>\n",
       "      <th>actor_1_facebook_likes</th>\n",
       "      <th>gross</th>\n",
       "      <th>genres</th>\n",
       "      <th>...</th>\n",
       "      <th>language</th>\n",
       "      <th>country</th>\n",
       "      <th>content_rating</th>\n",
       "      <th>budget</th>\n",
       "      <th>title_year</th>\n",
       "      <th>actor_2_facebook_likes</th>\n",
       "      <th>imdb_score</th>\n",
       "      <th>aspect_ratio</th>\n",
       "      <th>movie_facebook_likes</th>\n",
       "      <th>norm_movie_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4422</th>\n",
       "      <td>Color</td>\n",
       "      <td>Simeon Rice</td>\n",
       "      <td>6.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>Lisa Brave</td>\n",
       "      <td>393.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Action|Horror|Thriller</td>\n",
       "      <td>...</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>R</td>\n",
       "      <td>1500000.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>2.35</td>\n",
       "      <td>307</td>\n",
       "      <td>unsullied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>Color</td>\n",
       "      <td>Doug Liman</td>\n",
       "      <td>214.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>Ty Burrell</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>9528092.0</td>\n",
       "      <td>Biography|Drama|Thriller</td>\n",
       "      <td>...</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>22000000.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2.35</td>\n",
       "      <td>9000</td>\n",
       "      <td>fair game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3631</th>\n",
       "      <td>Color</td>\n",
       "      <td>Jonathan Levine</td>\n",
       "      <td>147.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>362.0</td>\n",
       "      <td>Aaron Yoo</td>\n",
       "      <td>976.0</td>\n",
       "      <td>2077046.0</td>\n",
       "      <td>Comedy|Drama|Romance</td>\n",
       "      <td>...</td>\n",
       "      <td>English</td>\n",
       "      <td>USA</td>\n",
       "      <td>R</td>\n",
       "      <td>6000000.0</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>617.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0</td>\n",
       "      <td>the wackness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      color    director_name  num_critic_for_reviews  duration  \\\n",
       "4422  Color      Simeon Rice                     6.0      93.0   \n",
       "1022  Color       Doug Liman                   214.0     108.0   \n",
       "3631  Color  Jonathan Levine                   147.0      99.0   \n",
       "\n",
       "      director_facebook_likes  actor_3_facebook_likes actor_2_name  \\\n",
       "4422                      6.0                    56.0   Lisa Brave   \n",
       "1022                    218.0                   405.0   Ty Burrell   \n",
       "3631                    129.0                   362.0    Aaron Yoo   \n",
       "\n",
       "      actor_1_facebook_likes      gross                    genres  \\\n",
       "4422                   393.0        NaN    Action|Horror|Thriller   \n",
       "1022                  6000.0  9528092.0  Biography|Drama|Thriller   \n",
       "3631                   976.0  2077046.0      Comedy|Drama|Romance   \n",
       "\n",
       "           ...        language country  content_rating      budget title_year  \\\n",
       "4422       ...         English     USA               R   1500000.0     2014.0   \n",
       "1022       ...         English     USA           PG-13  22000000.0     2010.0   \n",
       "3631       ...         English     USA               R   6000000.0     2008.0   \n",
       "\n",
       "      actor_2_facebook_likes imdb_score aspect_ratio  movie_facebook_likes  \\\n",
       "4422                   191.0        5.5         2.35                   307   \n",
       "1022                  3000.0        6.8         2.35                  9000   \n",
       "3631                   617.0        7.0         2.35                     0   \n",
       "\n",
       "     norm_movie_title  \n",
       "4422        unsullied  \n",
       "1022        fair game  \n",
       "3631     the wackness  \n",
       "\n",
       "[3 rows x 29 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggle_data.sample(3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data guides us to decide in what ways we might want to clean the data. For instance, the small sample data shown above, reveals that the **title_year** attribute is stored as floats (i.e., rational numbers). We can add another cleaning step to transform the **title_year** into strings and replace the missing title years with symbol **\"?\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>director_name</th>\n",
       "      <th>num_critic_for_reviews</th>\n",
       "      <th>duration</th>\n",
       "      <th>director_facebook_likes</th>\n",
       "      <th>actor_3_facebook_likes</th>\n",
       "      <th>actor_2_name</th>\n",
       "      <th>actor_1_facebook_likes</th>\n",
       "      <th>gross</th>\n",
       "      <th>genres</th>\n",
       "      <th>...</th>\n",
       "      <th>country</th>\n",
       "      <th>content_rating</th>\n",
       "      <th>budget</th>\n",
       "      <th>title_year</th>\n",
       "      <th>actor_2_facebook_likes</th>\n",
       "      <th>imdb_score</th>\n",
       "      <th>aspect_ratio</th>\n",
       "      <th>movie_facebook_likes</th>\n",
       "      <th>norm_movie_title</th>\n",
       "      <th>norm_title_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Color</td>\n",
       "      <td>James Cameron</td>\n",
       "      <td>723.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>855.0</td>\n",
       "      <td>Joel David Moore</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>760505847.0</td>\n",
       "      <td>Action|Adventure|Fantasy|Sci-Fi</td>\n",
       "      <td>...</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>237000000.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1.78</td>\n",
       "      <td>33000</td>\n",
       "      <td>avatar</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Color</td>\n",
       "      <td>Gore Verbinski</td>\n",
       "      <td>302.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>Orlando Bloom</td>\n",
       "      <td>40000.0</td>\n",
       "      <td>309404152.0</td>\n",
       "      <td>Action|Adventure|Fantasy</td>\n",
       "      <td>...</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>300000000.0</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>2.35</td>\n",
       "      <td>0</td>\n",
       "      <td>pirates of the caribbean: at worlds end</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Color</td>\n",
       "      <td>Sam Mendes</td>\n",
       "      <td>602.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>Rory Kinnear</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>200074175.0</td>\n",
       "      <td>Action|Adventure|Thriller</td>\n",
       "      <td>...</td>\n",
       "      <td>UK</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>245000000.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>393.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2.35</td>\n",
       "      <td>85000</td>\n",
       "      <td>spectre</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Color</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>813.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>Christian Bale</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>448130642.0</td>\n",
       "      <td>Action|Thriller</td>\n",
       "      <td>...</td>\n",
       "      <td>USA</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>250000000.0</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>2.35</td>\n",
       "      <td>164000</td>\n",
       "      <td>the dark knight rises</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Doug Walker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>131.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rob Walker</td>\n",
       "      <td>131.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>star wars: episode vii - the force awakens</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   color      director_name  num_critic_for_reviews  duration  \\\n",
       "0  Color      James Cameron                   723.0     178.0   \n",
       "1  Color     Gore Verbinski                   302.0     169.0   \n",
       "2  Color         Sam Mendes                   602.0     148.0   \n",
       "3  Color  Christopher Nolan                   813.0     164.0   \n",
       "4    NaN        Doug Walker                     NaN       NaN   \n",
       "\n",
       "   director_facebook_likes  actor_3_facebook_likes      actor_2_name  \\\n",
       "0                      0.0                   855.0  Joel David Moore   \n",
       "1                    563.0                  1000.0     Orlando Bloom   \n",
       "2                      0.0                   161.0      Rory Kinnear   \n",
       "3                  22000.0                 23000.0    Christian Bale   \n",
       "4                    131.0                     NaN        Rob Walker   \n",
       "\n",
       "   actor_1_facebook_likes        gross                           genres  \\\n",
       "0                  1000.0  760505847.0  Action|Adventure|Fantasy|Sci-Fi   \n",
       "1                 40000.0  309404152.0         Action|Adventure|Fantasy   \n",
       "2                 11000.0  200074175.0        Action|Adventure|Thriller   \n",
       "3                 27000.0  448130642.0                  Action|Thriller   \n",
       "4                   131.0          NaN                      Documentary   \n",
       "\n",
       "        ...       country content_rating       budget  title_year  \\\n",
       "0       ...           USA          PG-13  237000000.0      2009.0   \n",
       "1       ...           USA          PG-13  300000000.0      2007.0   \n",
       "2       ...            UK          PG-13  245000000.0      2015.0   \n",
       "3       ...           USA          PG-13  250000000.0      2012.0   \n",
       "4       ...           NaN            NaN          NaN         NaN   \n",
       "\n",
       "  actor_2_facebook_likes  imdb_score aspect_ratio movie_facebook_likes  \\\n",
       "0                  936.0         7.9         1.78                33000   \n",
       "1                 5000.0         7.1         2.35                    0   \n",
       "2                  393.0         6.8         2.35                85000   \n",
       "3                23000.0         8.5         2.35               164000   \n",
       "4                   12.0         7.1          NaN                    0   \n",
       "\n",
       "                             norm_movie_title norm_title_year  \n",
       "0                                      avatar            2009  \n",
       "1     pirates of the caribbean: at worlds end            2007  \n",
       "2                                     spectre            2015  \n",
       "3                       the dark knight rises            2012  \n",
       "4  star wars: episode vii - the force awakens               ?  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_year(year):\n",
    "    if pd.isnull(year):\n",
    "        return '?'\n",
    "    else:\n",
    "        return str(int(year))\n",
    "\n",
    "kaggle_data['norm_title_year'] = kaggle_data['title_year'].map(preprocess_year)\n",
    "kaggle_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Data Matching & Merging\n",
    "-------------------------\n",
    "The main goal in this part is go match the data that we have acquired from different sources to create a single rich dataset. Recall that in [Part 3](https://github.com/rit-git/BigGorilla/blob/tutorial/Tutorial/Part%203%20--%20Data%20Profiling%20%26%20Cleaning.ipynb), we transformed all datasets into a dataframe which we used to clean the data. In this part, we continue using the same dataframes for the data that we have prepared so far.\n",
    "\n",
    "### Step 1: Integrating the \"IMDB Plain Text Data\" files\n",
    "Note that both **ratings_data** and **genres_data** dataframes contain data that come from the same source (i.e., \"the IMDB Plain Text data\"). Thus, we assume that there are no inconsistencies between the data stored in these dataframe and to combine them, all we need to do is to match the entries that share the same title and production year. This simple \"exact match\" can be done simply using dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_x</th>\n",
       "      <th>year</th>\n",
       "      <th>rating</th>\n",
       "      <th>norm_movie</th>\n",
       "      <th>movie_y</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>1994</td>\n",
       "      <td>9.2</td>\n",
       "      <td>the shawshank redemption</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>1972</td>\n",
       "      <td>9.2</td>\n",
       "      <td>the godfather</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Godfather: Part II</td>\n",
       "      <td>1974</td>\n",
       "      <td>9.0</td>\n",
       "      <td>the godfather: part ii</td>\n",
       "      <td>The Godfather: Part II</td>\n",
       "      <td>Crime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>2008</td>\n",
       "      <td>8.9</td>\n",
       "      <td>the dark knight</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>Action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>1957</td>\n",
       "      <td>8.9</td>\n",
       "      <td>12 angry men</td>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>Crime</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     movie_x  year rating                norm_movie  \\\n",
       "0   The Shawshank Redemption  1994    9.2  the shawshank redemption   \n",
       "1              The Godfather  1972    9.2             the godfather   \n",
       "2     The Godfather: Part II  1974    9.0    the godfather: part ii   \n",
       "3            The Dark Knight  2008    8.9           the dark knight   \n",
       "4               12 Angry Men  1957    8.9              12 angry men   \n",
       "\n",
       "                    movie_y   genre  \n",
       "0  The Shawshank Redemption   Crime  \n",
       "1             The Godfather   Crime  \n",
       "2    The Godfather: Part II   Crime  \n",
       "3           The Dark Knight  Action  \n",
       "4              12 Angry Men   Crime  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brief_imdb_data = pd.merge(ratings_data, genres_data, how='inner', on=['norm_movie', 'year'])\n",
    "brief_imdb_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We refer to the dataset created above as the **brief_imdb_data** since it only contains two attributes (namely, genre and rating). Henceforth, we are going to use a richer version of the IMDB dataset which we created by integrating a number of files from the \"IMDB Plain Text Data\". If you have completed the first part of this tutorial, then this dataset is already downloaded and stored in *\"imdb_dataset.csv\"* under the _\"data\"_ folder. The following code snippet loads this dataset, does preprocessing on the title and production year of movies, removes the duplicates as before, and prints the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(869178, 27)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading the new IMDB dataset\n",
    "imdb_data = pd.read_csv('./data/imdb_dataset.csv')\n",
    "# let's normlize the title as we did in Part 3 of the tutorial\n",
    "imdb_data['norm_title'] = imdb_data['title'].map(preprocess_title)\n",
    "imdb_data['norm_year'] = imdb_data['year'].map(preprocess_year)\n",
    "imdb_data = imdb_data.drop_duplicates(subset=['norm_title', 'norm_year'], keep='first').copy()\n",
    "imdb_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Integrating the Kaggle and IMDB datasets\n",
    "\n",
    "A simple approach to integrate the two datasets is to simply join entries that share the same movie title and year of production. The following code reveals that 4,248 matches are found using this simple approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4248, 57)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_attempt1 = pd.merge(imdb_data, kaggle_data, how='inner', left_on=['norm_title', 'norm_year'],\n",
    "                         right_on=['norm_movie_title', 'norm_title_year'])\n",
    "data_attempt1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But given that IMDB and Kaggle datasets are collected from different sources, chances are that the name of a movie would be slightly different in these datasets (e.g. \"Wall.E\" vs \"WallE\"). To be able to find such matches, one can look at the similarity of movie titles and consider title with high similarity to be the same entity. BigGorilla's recommendation for doing similarity join across two datasets is the python package **py_stringsimjoin**. The following code snippet uses the **py_stringsimjoin** to match all the titles that have an edit distance of one or less (i.e., there is at most one character that needs to be changed/added/removed to make both titles identical). Once the similarity join is complete, it only selects the title pairs that are produced in the same year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:02:46\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4689, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import py_stringsimjoin as ssj\n",
    "import py_stringmatching as sm\n",
    "\n",
    "imdb_data['id'] = range(imdb_data.shape[0])\n",
    "kaggle_data['id'] = range(kaggle_data.shape[0])\n",
    "similar_titles = ssj.edit_distance_join(imdb_data, kaggle_data, 'id', 'id', 'norm_title',\n",
    "                                        'norm_movie_title', l_out_attrs=['norm_title', 'norm_year'],\n",
    "                                         r_out_attrs=['norm_movie_title', 'norm_title_year'], threshold=1)\n",
    "# selecting the entries that have the same production year\n",
    "data_attempt2 = similar_titles[similar_titles.r_norm_title_year == similar_titles.l_norm_year]\n",
    "data_attempt2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that using the similarity join 4,689 titles were matched. Let's look at some of the titles that are matched by the similarity join but are not identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>l_id</th>\n",
       "      <th>r_id</th>\n",
       "      <th>l_norm_title</th>\n",
       "      <th>l_norm_year</th>\n",
       "      <th>r_norm_movie_title</th>\n",
       "      <th>r_norm_title_year</th>\n",
       "      <th>_sim_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>144</td>\n",
       "      <td>852736</td>\n",
       "      <td>46</td>\n",
       "      <td>world war v</td>\n",
       "      <td>2013</td>\n",
       "      <td>world war z</td>\n",
       "      <td>2013</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>162</td>\n",
       "      <td>281649</td>\n",
       "      <td>56</td>\n",
       "      <td>grave</td>\n",
       "      <td>2012</td>\n",
       "      <td>brave</td>\n",
       "      <td>2012</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>180</td>\n",
       "      <td>831490</td>\n",
       "      <td>58</td>\n",
       "      <td>walle</td>\n",
       "      <td>2008</td>\n",
       "      <td>wall·e</td>\n",
       "      <td>2008</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>236</td>\n",
       "      <td>816188</td>\n",
       "      <td>67</td>\n",
       "      <td>upe</td>\n",
       "      <td>2009</td>\n",
       "      <td>up</td>\n",
       "      <td>2009</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>243</td>\n",
       "      <td>817366</td>\n",
       "      <td>67</td>\n",
       "      <td>ut</td>\n",
       "      <td>2009</td>\n",
       "      <td>up</td>\n",
       "      <td>2009</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     _id    l_id  r_id l_norm_title l_norm_year r_norm_movie_title  \\\n",
       "144  144  852736    46  world war v        2013        world war z   \n",
       "162  162  281649    56        grave        2012              brave   \n",
       "180  180  831490    58        walle        2008             wall·e   \n",
       "236  236  816188    67          upe        2009                 up   \n",
       "243  243  817366    67           ut        2009                 up   \n",
       "\n",
       "    r_norm_title_year  _sim_score  \n",
       "144              2013         1.0  \n",
       "162              2012         1.0  \n",
       "180              2008         1.0  \n",
       "236              2009         1.0  \n",
       "243              2009         1.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_attempt2[data_attempt2.l_norm_title != data_attempt2.r_norm_movie_title].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While instances such as \"walle\" and \"wall.e\" are correctly matched, we can see that this techniques also makes some errors (e.g., \"grave\" and \"brave\"). This raises the following questions: \"what method should be used for data matching?\" and \"how can we determine the quality of the matching?\". BigGorilla's recommendation for dealing with this problem is using the pythong package **py_entitymatching** which is developed as part of the [Magellan project](https://sites.google.com/site/anhaidgroup/projects/magellan).\n",
    "\n",
    "In the next step, we demonstrate how **py_entitymatching** uses machine learning techniques for the data-matching purposes as well as how it enables us to evaluate the quality of the produced matching.\n",
    "\n",
    "### Step 3: Using Magellan for data matching\n",
    "\n",
    "#### Substep A: Finding a candiate set (Blocking)\n",
    "The goal of this step is to limit the number of pairs that we consider as potential matches using a simple heuristic. For this task, we can create a new column in each dataset that combines the values of important attributes into a single string (which we call the **mixture**). Then, we can use the string similarity join as before to find a set of entities that have some overlap in the values of the important columns. Before doing that, we need to transform the columns that are part of the mixture to strings. The **py_stringsimjoin** package allows us to do so easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the \"budget\" column into string and creating a new **mixture** column\n",
    "ssj.utils.converter.dataframe_column_to_str(imdb_data, 'budget', inplace=True)\n",
    "imdb_data['mixture'] = imdb_data['norm_title'] + ' ' + imdb_data['norm_year'] + ' ' + imdb_data['budget']\n",
    "\n",
    "# repeating the same thing for the Kaggle dataset\n",
    "ssj.utils.converter.dataframe_column_to_str(kaggle_data, 'budget', inplace=True)\n",
    "kaggle_data['mixture'] = kaggle_data['norm_movie_title'] + ' ' + kaggle_data['norm_title_year'] + \\\n",
    "                         ' ' + kaggle_data['budget']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the **mixture** columns to create a desired candiate set which we call **C**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18317, 14)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = ssj.overlap_coefficient_join(kaggle_data, imdb_data, 'id', 'id', 'mixture', 'mixture', sm.WhitespaceTokenizer(), \n",
    "                                 l_out_attrs=['norm_movie_title', 'norm_title_year', 'duration',\n",
    "                                              'budget', 'content_rating'],\n",
    "                                 r_out_attrs=['norm_title', 'norm_year', 'length', 'budget', 'mpaa'],\n",
    "                                 threshold=0.65)\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by doing a similarity join, we already reduced the candidate set to 18,317 pairs.\n",
    "\n",
    "#### Substep B: Specifying the keys \n",
    "The next step is to specify to the **py_entitymatching** package which columns correspond to the keys in each dataframe. Also, we need to specify which columns correspond to the foreign keys of the the two dataframes in the candidate set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lexingtong/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import py_entitymatching as em\n",
    "em.set_key(kaggle_data, 'id')   # specifying the key column in the kaggle dataset\n",
    "em.set_key(imdb_data, 'id')     # specifying the key column in the imdb dataset\n",
    "em.set_key(C, '_id')            # specifying the key in the candidate set\n",
    "em.set_ltable(C, kaggle_data)   # specifying the left table \n",
    "em.set_rtable(C, imdb_data)     # specifying the right table\n",
    "em.set_fk_rtable(C, 'r_id')     # specifying the column that matches the key in the right table \n",
    "em.set_fk_ltable(C, 'l_id')     # specifying the column that matches the key in the left table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Subset C: Debugging the blocker\n",
    "\n",
    "Now, we need to make sure that the candidate set is loose enough to include pairs of movies that are not very close. If this is not the case, there is a chance that we have eliminated pair that could be potentially matched together. By looking at a few pairs from the candidate set, we can judge whether the blocking step has been too harsh or not.\n",
    "\n",
    "*Note: The **py_entitymatching** package provides some tools for debugging the blocker as well.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l_norm_movie_title</th>\n",
       "      <th>r_norm_title</th>\n",
       "      <th>l_norm_title_year</th>\n",
       "      <th>r_norm_year</th>\n",
       "      <th>l_budget</th>\n",
       "      <th>r_budget</th>\n",
       "      <th>l_content_rating</th>\n",
       "      <th>r_mpaa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dude  wheres my dog!</td>\n",
       "      <td>#hacked</td>\n",
       "      <td>2014</td>\n",
       "      <td>2014</td>\n",
       "      <td>20000</td>\n",
       "      <td>20000</td>\n",
       "      <td>PG</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>road hard</td>\n",
       "      <td>#horror</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>1500000</td>\n",
       "      <td>1500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>me you and five bucks</td>\n",
       "      <td>#horror</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>1500000</td>\n",
       "      <td>1500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>checkmate</td>\n",
       "      <td>#horror</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>1500000</td>\n",
       "      <td>1500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#horror</td>\n",
       "      <td>#horror</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "      <td>1500000</td>\n",
       "      <td>1500000</td>\n",
       "      <td>Not Rated</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      l_norm_movie_title r_norm_title l_norm_title_year r_norm_year l_budget  \\\n",
       "0   dude  wheres my dog!      #hacked              2014        2014    20000   \n",
       "1              road hard      #horror              2015        2015  1500000   \n",
       "2  me you and five bucks      #horror              2015        2015  1500000   \n",
       "3              checkmate      #horror              2015        2015  1500000   \n",
       "4                #horror      #horror              2015        2015  1500000   \n",
       "\n",
       "  r_budget l_content_rating r_mpaa  \n",
       "0    20000               PG    NaN  \n",
       "1  1500000              NaN    NaN  \n",
       "2  1500000              NaN    NaN  \n",
       "3  1500000              NaN    NaN  \n",
       "4  1500000        Not Rated    NaN  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[['l_norm_movie_title', 'r_norm_title', 'l_norm_title_year', 'r_norm_year',\n",
    "   'l_budget', 'r_budget', 'l_content_rating', 'r_mpaa']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above sample we can see that the blocking seems to be reasonable.\n",
    "\n",
    "#### Substep D: Sampling from the candiate set\n",
    "\n",
    "The goal of this step is to obtain a sample from the candidate set and manually label the sampled candidates; that is, to specify if the candiate pair is a correct match or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling 500 pairs and writing this sample into a .csv file\n",
    "sampled = C.sample(500, random_state=0)\n",
    "sampled.to_csv('./data/sampled.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to label the sampled data, we can create a new column in the _.csv_ file (which we call **label**) and put value 1 under that column if the pair is a correct match and 0 otherwise. To avoid overriding the files, let's rename the new file as **labeled.csv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"py_entitymatching.io.parsers\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>_id</th>\n",
       "      <th>l_id</th>\n",
       "      <th>r_id</th>\n",
       "      <th>l_norm_movie_title</th>\n",
       "      <th>l_norm_title_year</th>\n",
       "      <th>l_duration</th>\n",
       "      <th>l_budget</th>\n",
       "      <th>l_content_rating</th>\n",
       "      <th>r_norm_title</th>\n",
       "      <th>r_norm_year</th>\n",
       "      <th>r_length</th>\n",
       "      <th>r_budget</th>\n",
       "      <th>r_mpaa</th>\n",
       "      <th>_sim_score</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4771</td>\n",
       "      <td>4771</td>\n",
       "      <td>2639</td>\n",
       "      <td>235925</td>\n",
       "      <td>eye of the beholder</td>\n",
       "      <td>1999</td>\n",
       "      <td>109.0</td>\n",
       "      <td>15000000</td>\n",
       "      <td>R</td>\n",
       "      <td>eye of the beholder</td>\n",
       "      <td>1999</td>\n",
       "      <td>109.0</td>\n",
       "      <td>35000000</td>\n",
       "      <td>R</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11478</td>\n",
       "      <td>11478</td>\n",
       "      <td>2001</td>\n",
       "      <td>600301</td>\n",
       "      <td>rocky balboa</td>\n",
       "      <td>2006</td>\n",
       "      <td>139.0</td>\n",
       "      <td>24000000</td>\n",
       "      <td>PG</td>\n",
       "      <td>rocky balboa</td>\n",
       "      <td>2006</td>\n",
       "      <td>139.0</td>\n",
       "      <td>24000000</td>\n",
       "      <td>PG</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13630</td>\n",
       "      <td>13630</td>\n",
       "      <td>4160</td>\n",
       "      <td>691766</td>\n",
       "      <td>from russia with love</td>\n",
       "      <td>1963</td>\n",
       "      <td>115.0</td>\n",
       "      <td>2000000</td>\n",
       "      <td>Approved</td>\n",
       "      <td>the aeolians: from russia with love</td>\n",
       "      <td>2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1972</td>\n",
       "      <td>1972</td>\n",
       "      <td>1248</td>\n",
       "      <td>101029</td>\n",
       "      <td>sex tape</td>\n",
       "      <td>2014</td>\n",
       "      <td>94.0</td>\n",
       "      <td>40000000</td>\n",
       "      <td>R</td>\n",
       "      <td>blended</td>\n",
       "      <td>2014</td>\n",
       "      <td>117.0</td>\n",
       "      <td>40000000</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15903</td>\n",
       "      <td>15903</td>\n",
       "      <td>722</td>\n",
       "      <td>758133</td>\n",
       "      <td>the scorch trials</td>\n",
       "      <td>2015</td>\n",
       "      <td>132.0</td>\n",
       "      <td>61000000</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>the scorch trials</td>\n",
       "      <td>2015</td>\n",
       "      <td>132.0</td>\n",
       "      <td>61000000</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    _id  l_id    r_id     l_norm_movie_title  l_norm_title_year  \\\n",
       "0        4771   4771  2639  235925    eye of the beholder               1999   \n",
       "1       11478  11478  2001  600301           rocky balboa               2006   \n",
       "2       13630  13630  4160  691766  from russia with love               1963   \n",
       "3        1972   1972  1248  101029               sex tape               2014   \n",
       "4       15903  15903   722  758133      the scorch trials               2015   \n",
       "\n",
       "   l_duration  l_budget l_content_rating                         r_norm_title  \\\n",
       "0       109.0  15000000                R                  eye of the beholder   \n",
       "1       139.0  24000000               PG                         rocky balboa   \n",
       "2       115.0   2000000         Approved  the aeolians: from russia with love   \n",
       "3        94.0  40000000                R                              blended   \n",
       "4       132.0  61000000            PG-13                    the scorch trials   \n",
       "\n",
       "   r_norm_year  r_length  r_budget r_mpaa  _sim_score  label  \n",
       "0         1999     109.0  35000000      R    0.833333      1  \n",
       "1         2006     139.0  24000000     PG    1.000000      1  \n",
       "2         2012       NaN     20000    NaN    0.666667      0  \n",
       "3         2014     117.0  40000000  PG-13    0.666667      0  \n",
       "4         2015     132.0  61000000  PG-13    1.000000      1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you would like to avoid labeling the pairs for now, you can download the labled.csv file from\n",
    "# BigGorilla using the following command (if you prefer to do it yourself, commend the next line)\n",
    "response = urllib.urlretrieve('https://anaconda.org/BigGorilla/datasets/1/download/labeled.csv',\n",
    "                              './data/labeled.csv')\n",
    "labeled = em.read_csv_metadata('data/labeled.csv', ltable=kaggle_data, rtable=imdb_data,\n",
    "                               fk_ltable='l_id', fk_rtable='r_id', key='_id')\n",
    "labeled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Substep E: Traning machine learning algorithms\n",
    "\n",
    "Now we can use the sampled dataset to train various machine learning algorithms for our prediction task. To do so, we need to split our dataset into a training and a test set, and then select the desired machine learning techniques for our prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split = em.split_train_test(labeled, train_proportion=0.5, random_state=0)\n",
    "train_data = split['train']\n",
    "test_data = split['test']\n",
    "\n",
    "dt = em.DTMatcher(name='DecisionTree', random_state=0)\n",
    "svm = em.SVMMatcher(name='SVM', random_state=0)\n",
    "rf = em.RFMatcher(name='RF', random_state=0)\n",
    "lg = em.LogRegMatcher(name='LogReg', random_state=0)\n",
    "ln = em.LinRegMatcher(name='LinReg')\n",
    "nb = em.NBMatcher(name='NaiveBayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can apply any machine learning technique, we need to extract a set of features. Fortunately, the **py_entitymatching** package can automatically extract a set of features once we specify which columns in the two datasets correspond to each other. The following code snippet starts by specifying the correspondence between the column of the two datasets. Then, it uses the **py_entitymatching** package to determine the type of each column. By considering the types of columns in each dataset (stored in variables *l_attr_types* and *r_attr_types*), and using the tokenizers and similarity functions suggested by the package, we can extract a set of instructions for extracting features. Note that variable **F** is not the set of extracted features, rather it encodes the instructions for computing the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attr_corres = em.get_attr_corres(kaggle_data, imdb_data)\n",
    "attr_corres['corres'] = [('norm_movie_title', 'norm_title'), \n",
    "                         ('norm_title_year', 'norm_year'),\n",
    "                        ('content_rating', 'mpaa'),\n",
    "                         ('budget', 'budget'),\n",
    "]\n",
    "\n",
    "l_attr_types = em.get_attr_types(kaggle_data)\n",
    "r_attr_types = em.get_attr_types(imdb_data)\n",
    "\n",
    "tok = em.get_tokenizers_for_matching()\n",
    "sim = em.get_sim_funs_for_matching()\n",
    "\n",
    "F = em.get_features(kaggle_data, imdb_data, l_attr_types, r_attr_types, attr_corres, tok, sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the set of desired features **F**, we can now calculate the feature values for our training data and also impute the missing values in our data. In this case, we choose to replace the missing values with the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = em.extract_feature_vecs(train_data, feature_table=F, attrs_after='label', show_progress=False) \n",
    "train_features = em.impute_table(train_features,  exclude_attrs=['_id', 'l_id', 'r_id', 'label'], strategy='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the calculated features, we can evaluate the performance of different machine learning algorithms and select the best one for our matching task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lexingtong/miniconda2/lib/python2.7/site-packages/scipy/linalg/basic.py:884: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Matcher</th>\n",
       "      <th>Num folds</th>\n",
       "      <th>Fold 1</th>\n",
       "      <th>Fold 2</th>\n",
       "      <th>Fold 3</th>\n",
       "      <th>Fold 4</th>\n",
       "      <th>Fold 5</th>\n",
       "      <th>Mean score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>&lt;py_entitymatching.matcher.dtmatcher.DTMatcher object at 0x15d828090&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.993548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RF</td>\n",
       "      <td>&lt;py_entitymatching.matcher.rfmatcher.RFMatcher object at 0x15d828550&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.993548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>&lt;py_entitymatching.matcher.svmmatcher.SVMMatcher object at 0x15d8284d0&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.959853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LinReg</td>\n",
       "      <td>&lt;py_entitymatching.matcher.linregmatcher.LinRegMatcher object at 0x15d8560d0&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.993548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>&lt;py_entitymatching.matcher.logregmatcher.LogRegMatcher object at 0x15d8281d0&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.984853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>&lt;py_entitymatching.matcher.nbmatcher.NBMatcher object at 0x111b2c290&gt;</td>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.993548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name  \\\n",
       "0  DecisionTree   \n",
       "1            RF   \n",
       "2           SVM   \n",
       "3        LinReg   \n",
       "4        LogReg   \n",
       "5    NaiveBayes   \n",
       "\n",
       "                                                                         Matcher  \\\n",
       "0          <py_entitymatching.matcher.dtmatcher.DTMatcher object at 0x15d828090>   \n",
       "1          <py_entitymatching.matcher.rfmatcher.RFMatcher object at 0x15d828550>   \n",
       "2        <py_entitymatching.matcher.svmmatcher.SVMMatcher object at 0x15d8284d0>   \n",
       "3  <py_entitymatching.matcher.linregmatcher.LinRegMatcher object at 0x15d8560d0>   \n",
       "4  <py_entitymatching.matcher.logregmatcher.LogRegMatcher object at 0x15d8281d0>   \n",
       "5          <py_entitymatching.matcher.nbmatcher.NBMatcher object at 0x111b2c290>   \n",
       "\n",
       "   Num folds    Fold 1    Fold 2  Fold 3    Fold 4  Fold 5  Mean score  \n",
       "0          5  1.000000  0.967742     1.0  1.000000   1.000    0.993548  \n",
       "1          5  1.000000  0.967742     1.0  1.000000   1.000    0.993548  \n",
       "2          5  0.956522  0.967742     1.0  1.000000   0.875    0.959853  \n",
       "3          5  1.000000  0.967742     1.0  1.000000   1.000    0.993548  \n",
       "4          5  1.000000  0.967742     1.0  0.956522   1.000    0.984853  \n",
       "5          5  1.000000  0.967742     1.0  1.000000   1.000    0.993548  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = em.select_matcher([dt, rf, svm, ln, lg, nb], table=train_features, \n",
    "                           exclude_attrs=['_id', 'l_id', 'r_id', 'label'], k=5,\n",
    "                           target_attr='label', metric='f1', random_state=0)\n",
    "result['cv_stats']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe based on the reported accuracy of different techniques that the \"random forest (RF)\" algorithm achieves the best performance. Thus, it is best to use this technique for the matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Substep F: Evaluating the quality of our matching\n",
    "\n",
    "It is important to evaluate the quality of our matching. We can now, use the traning set for this purpose and measure how well the random forest predicts the matches. We can see that we are obtaining a high accuracy and recall on the test set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 94.44% (51/54)\n",
      "Recall : 100.0% (51/51)\n",
      "F1 : 97.14%\n",
      "False positives : 3 (out of 54 positive predictions)\n",
      "False negatives : 0 (out of 196 negative predictions)\n"
     ]
    }
   ],
   "source": [
    "best_model = result['selected_matcher']\n",
    "best_model.fit(table=train_features, exclude_attrs=['_id', 'l_id', 'r_id', 'label'], target_attr='label')\n",
    "\n",
    "test_features = em.extract_feature_vecs(test_data, feature_table=F, attrs_after='label', show_progress=False)\n",
    "test_features = em.impute_table(test_features, exclude_attrs=['_id', 'l_id', 'r_id', 'label'], strategy='mean')\n",
    "\n",
    "# Predict on the test data\n",
    "predictions = best_model.predict(table=test_features, exclude_attrs=['_id', 'l_id', 'r_id', 'label'], \n",
    "                                 append=True, target_attr='predicted', inplace=False)\n",
    "\n",
    "# Evaluate the predictions\n",
    "eval_result = em.eval_matches(predictions, 'label', 'predicted')\n",
    "em.print_eval_summary(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Substep G: Using the trained model to match the datasets\n",
    "\n",
    "Now, we can use the trained model to match the two tables as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:05\n"
     ]
    }
   ],
   "source": [
    "candset_features = em.extract_feature_vecs(C, feature_table=F, show_progress=True)\n",
    "candset_features = em.impute_table(candset_features, exclude_attrs=['_id', 'l_id', 'r_id'], strategy='mean')\n",
    "predictions = best_model.predict(table=candset_features, exclude_attrs=['_id', 'l_id', 'r_id'],\n",
    "                                 append=True, target_attr='predicted', inplace=False)\n",
    "matches = predictions[predictions.predicted == 1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the **matches** dataframe contains many columns storing the extracted features for both datasets. The following code snippet removes all the unnecessary columns and creates a nice formatted dataframe that has the resulting integrated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>l_id</th>\n",
       "      <th>r_id</th>\n",
       "      <th>l_norm_movie_title</th>\n",
       "      <th>l_norm_title_year</th>\n",
       "      <th>l_budget</th>\n",
       "      <th>l_content_rating</th>\n",
       "      <th>r_norm_title</th>\n",
       "      <th>r_norm_year</th>\n",
       "      <th>r_budget</th>\n",
       "      <th>r_mpaa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>4352</td>\n",
       "      <td>106</td>\n",
       "      <td>#horror</td>\n",
       "      <td>2015</td>\n",
       "      <td>1500000</td>\n",
       "      <td>Not Rated</td>\n",
       "      <td>#horror</td>\n",
       "      <td>2015</td>\n",
       "      <td>1500000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>2726</td>\n",
       "      <td>450</td>\n",
       "      <td>crocodile dundee ii</td>\n",
       "      <td>1988</td>\n",
       "      <td>15800000</td>\n",
       "      <td>PG</td>\n",
       "      <td>crocodile dundee ii</td>\n",
       "      <td>1988</td>\n",
       "      <td>14000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>3406</td>\n",
       "      <td>838</td>\n",
       "      <td>500 days of summer</td>\n",
       "      <td>2009</td>\n",
       "      <td>7500000</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>(500) days of summer</td>\n",
       "      <td>2009</td>\n",
       "      <td>7500000</td>\n",
       "      <td>PG-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>3631</td>\n",
       "      <td>1872</td>\n",
       "      <td>10 cloverfield lane</td>\n",
       "      <td>2016</td>\n",
       "      <td>15000000</td>\n",
       "      <td>PG-13</td>\n",
       "      <td>10 cloverfield lane</td>\n",
       "      <td>2016</td>\n",
       "      <td>15000000</td>\n",
       "      <td>PG-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26</td>\n",
       "      <td>2965</td>\n",
       "      <td>1881</td>\n",
       "      <td>10 days in a madhouse</td>\n",
       "      <td>2015</td>\n",
       "      <td>12000000</td>\n",
       "      <td>R</td>\n",
       "      <td>10 days in delaware</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _id  l_id  r_id     l_norm_movie_title l_norm_title_year  l_budget  \\\n",
       "0    4  4352   106                #horror              2015   1500000   \n",
       "1    8  2726   450    crocodile dundee ii              1988  15800000   \n",
       "2   11  3406   838     500 days of summer              2009   7500000   \n",
       "3   24  3631  1872    10 cloverfield lane              2016  15000000   \n",
       "4   26  2965  1881  10 days in a madhouse              2015  12000000   \n",
       "\n",
       "  l_content_rating          r_norm_title r_norm_year  r_budget r_mpaa  \n",
       "0        Not Rated               #horror        2015   1500000    NaN  \n",
       "1               PG   crocodile dundee ii        1988  14000000    NaN  \n",
       "2            PG-13  (500) days of summer        2009   7500000  PG-13  \n",
       "3            PG-13   10 cloverfield lane        2016  15000000  PG-13  \n",
       "4                R   10 days in delaware        2015         0    NaN  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from py_entitymatching.catalog import catalog_manager as cm\n",
    "matches = matches[['_id', 'l_id', 'r_id', 'predicted']]\n",
    "matches.reset_index(drop=True, inplace=True)\n",
    "cm.set_candset_properties(matches, '_id', 'l_id', 'r_id', kaggle_data, imdb_data)\n",
    "matches = em.add_output_attributes(matches, l_output_attrs=['norm_movie_title', 'norm_title_year', 'budget', 'content_rating'],\n",
    "                                   r_output_attrs=['norm_title', 'norm_year', 'budget', 'mpaa'],\n",
    "                                   l_output_prefix='l_', r_output_prefix='r_',\n",
    "                                   delete_from_catalog=False)\n",
    "matches.drop('predicted', axis=1, inplace=True)\n",
    "matches.head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
